# **AlphaExaAI**
_Distributed Deep Learning Simulator & Benchmarking Framework for Exascale AI Systems_

![Python](https://img.shields.io/badge/python-3.10+-blue.svg?logo=python)
![PyTorch](https://img.shields.io/badge/PyTorch-2.3+-ee4c2c.svg?logo=pytorch)
![CUDA](https://img.shields.io/badge/CUDA-11.8-green.svg?logo=nvidia)
![MPI](https://img.shields.io/badge/MPI4Py-enabled-orange.svg)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
![Status](https://img.shields.io/badge/status-active-success.svg)

---
#ProjectaExaAI â€” 250B Exascale Foundation Model**
**Next-generation open-source agentic intelligence designed for exascale supercomputing systems.**  
Built to surpass Gemini 3, Claude Sonnet 4.5, GPT-5 class models in reasoning, planning, scientific analysis, and code execution.

---

<p align="center">
  <img src="https://www.pngkey.com/png/full/894-8943983_artificial-intelligence-png.png"130">
</p>

---

##  Overview

**AlphaExaAI** is a large-scale foundation model (**250B parameters**) engineered for exascale distributed training across GPU supercomputers.  
It combines:

- **Hybrid Mixture-of-Experts (MoE) + Dense Transformers**
- **Agentic Intelligence Layer**
- **1M-token context window**
- **Scientific reasoning modules**
- **Fully distributed execution across 64â€“512 GPUs**

AlphaExaAI aims to be the **strongest open-source agentic AI framework in the world**, optimized for HPC centers such as:

- Polaris (A100)
- Aurora
- ILABT
- EGI Federation
- LUMI
- EuroHPC systems

---

## ğŸŒŸ Project Mission

AlphaExaAI is designed to:

1. Deliver an *exascale-ready* open-source AI model at 250B parameters  
2. Provide an **agentic system** capable of tool use, planning, coding, mathematics, and physics  
3. Offer a **transparent training blueprint** for HPC researchers  
4. Release **public benchmarks, datasets, logs, and scaling results**  
5. Act as a research-grade alternative to closed frontier models  

---

# ğŸ”— Official Resources

| Item | Link |
|------|------|
| ğŸ“¦ GitHub Repository | https://github.com/hleliofficiel/AlphaExaAI |
| ğŸŒ Website (activation pending) | https://alphaexa.ai |
| ğŸ“š Documentation | `docs/` folder |
| ğŸ“ Technical Specification | `docs/AlphaExaAI_Model_Spec_250B.md` |
| ğŸ“ˆ Benchmarks | `docs/Benchmarks_Initial.md` |
| âš™ï¸ Scaling Guide | `docs/Scaling_Strategy_Polaris_Aurora.md` |

---

# ğŸ§¬ Model Architecture (High Overview)

AlphaExaAI-250B uses a **Hybrid MoE Transformer** featuring:

- **250B total parameters**
- 32 MoE experts (6.5B each)
- 120 transformer layers
- 128 attention heads
- Extended RoPE positional embeddings
- 1M token context capability
- FlashAttention-3 kernels
- Reinforced planning module (AgentCore)

<p align="center">
  <img src="https://camo.githubusercontent.com/4d76569b4d17ab35fbf2de42aa02b56a65ad0549812be2c96875dcde39824ece/68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f68756767696e67666163652f646f63756d656e746174696f6e2d696d616765732f7261772f6d61696e2f7472616e73666f726d6572732d6c6f676f2d6461726b2e737667" width="380">
</p>

---

# ğŸ”¥ Key Capabilities

### ğŸ§  **Agentic Intelligence**
- Multi-step planning  
- Tool + API calling  
- Memory optimization  
- Code execution engine  
- Autonomous research workflows  

### ğŸ“˜ **Scientific & Mathematical Reasoning**
- Physics modeling  
- Equation solving  
- Symbolic algebra  
- Multi-document analysis  

### ğŸ–¥ **Exascale Optimization**
- FSDP  
- ZeRO-3  
- MoE parallelism  
- Tensor + pipeline parallel  
- Recovery-aware checkpointing  

---

# ğŸ“Š Expected Performance Targets

| Capability | Expected Level | Equivalent To |
|------------|----------------|----------------|
| Code Reasoning | Very High | GPT-5, Sonnet 4.5 |
| Math & Logic | Extremely High | Gemini 3 Pro |
| Long Context | 1,000,000 tokens | Claude 3.5 class |
| Multi-Agent Ops | Advanced | GPT-5 / DeepSeek-R1 |
| Physics & Engineering | HPC-enhanced | Research-grade |

---
# ğŸ— Repository Structure
AlphaExaAI/ â”‚ â”œâ”€â”€ docs/ â”‚   â”œâ”€â”€ AlphaExaAI_Model_Spec_250B.md â”‚   â”œâ”€â”€ Architecture_Details.md â”‚   â”œâ”€â”€ Training_Plan_Exascale.md â”‚   â”œâ”€â”€ Scaling_Strategy_Polaris_Aurora.md â”‚   â”œâ”€â”€ Benchmarks_Initial.md â”‚   â””â”€â”€ Research_Objectives.md â”‚ â”œâ”€â”€ src/ â”‚   â”œâ”€â”€ modeling/ â”‚   â”œâ”€â”€ agents/ â”‚   â”œâ”€â”€ training/ â”‚   â”œâ”€â”€ data/ â”‚   â””â”€â”€ utils/ â”‚ â”œâ”€â”€ scripts/ â”‚   â”œâ”€â”€ run_distributed.sh â”‚   â””â”€â”€ slurm/ â”‚ â”œâ”€â”€ requirements.txt â””â”€â”€ README.md

---

# âš¡ Why HPC Resources Are Required

AlphaExaAI needs supercomputing power because it involves:

- Training a **250B parameter MoE foundation model**  
- Running **multi-node distributed experiments**  
- 24/7 fault-tolerant long runs  
- 50GB+ checkpoints  
- 15T+ training tokens  
- High-speed interconnect (>1 TB/s aggregated)  

Cloud systems cannot support sustained training of this scale â€” HPC is mandatory.

---

# ğŸ“ˆ Early Benchmarks (Prototype)

| Model | GPUs | Throughput | Notes |
|-------|------|------------|--------|
| 1B prototype | 32 A100 | 220K tok/s | Stable |
| 7B prototype | 64 A100 | 680K tok/s | Full attention |
| 30B prototype | 128 A100 | 1.4M tok/s | Distributed stable |
| 250B (target) | 1024 A100 | ~4.0M tok/s | Projection |

Full results in:  
`docs/Benchmarks_Initial.md`

---

# ğŸ¯ Roadmap

### **Phase 1 â€” Core Development (done)**
- Tokenizer  
- Dataset processing  
- Prototype 1B/7B models  

### **Phase 2 â€” Distributed Scaling (in progress)**
- NCCL + FSDP  
- Multi-node MoE  

### **Phase 3 â€” Full AlphaExaAI-250B Training**
- 64 â†’ 256 nodes  
- 24/7 training  
- Evaluation + RLHF  

### **Phase 4 â€” Release**
- Public inference API  
- Model weights  
- Web demo  
- Benchmark papers  

---

# ğŸ‘¤ Project Lead & Contact

- **Lead Researcher:** *Mohammed Hleli*  
- **Email:** *h.hleli@tuta.io*  
- **GitHub:** https://github.com/hleliofficiel  
- **Location:** Tunisia â€” HPC Open Researcher  

---

<p align="center">
  <img src="https://i0.wp.com/www.wi6labs.com/wp-content/uploads/2019/12/Machine-learning-logo-1.png" width="150">
</p>

---

# ğŸŸ¦ License

This project is released under the **MIT License**.  
All contributions and research outputs remain open to the scientific community.

---

**AlphaExaAI â€” The future of open, exascale, agentic intelligence.**
