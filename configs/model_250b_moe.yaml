model_type: "alphaexa_moe"
vocab_size: 128256  # Extended vocabulary for code/math
hidden_size: 16384
num_hidden_layers: 128
num_attention_heads: 128
num_key_value_heads: 16  # GQA (Grouped Query Attention) for speed
intermediate_size: 44032
max_position_embeddings: 2097152  # 2M Context Window
rope_theta: 5000000.0  # High theta for long context stability

# Mixture of Experts (MoE) Config
moe:
  num_experts_per_tok: 2  # Active experts
  num_local_experts: 64   # Total experts
  router_aux_loss_coef: 0.02
  capacity_factor: 1.25
  expert_layer_period: 2  # Apply MoE every 2nd layer
  expert_parallelism: true

# Agentic Optimization
agent_token_id: 128003  # Special token for <agent_action>
thought_token_id: 128004 # Special token for <reasoning>

# Training Stability
initialization:
  std: 0.006
  range_matrices_init: true
norm_type: "rmsnorm"
rms_norm_eps: 1e-5
use_flash_attn: true
flash_attn_version: "v3"
